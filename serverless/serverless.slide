Gettin' Jiggy with Serverless 

A closer look at world of serverless
Tags: serverless, lambda, aws

Douglas Chimento
Outbrain Inc.
dchimento@outbrain.com
https://presentations.dougchimento.com/serverless/serverless.slide

* What we want - The 80/20 Rule

- We want a simple way to deploy our code
- We want to focus on core biz logic
- We don't want to worry about scaling
- We don't care about the OS
- We want automatic fault tolerance
- Most of the time our code executes quickly (< 100ms)
- Responds to HTTP events (requests)

* What does serverless mean?
- No servers to provision or manage
- Scale with usage
- Built in fault-tolerance
- Never pay for idle/unused capacity
- Event driven
: No Idele servers
: Serverless should be called event driven architecture 

* Lexicon
- Paas - Platform as a Service
- Baas - Backend as a service
- FaaS - Function as a service

The combination of these concepts are *serverless*

* _
.background evolution.jpg
: Ops and Finance mostly benefit from VM

* Componets of serverless
.background lambda_in_action.png
.html lambda_in_action.html

* A view on Serverless
.html twitter.html

* Mental shift
- Microservices to the extreme
- Stateless to the extreme
- Async is (mostly) irrelevant
- Focus on one task and do it well
- Event driven programming

* Extreme Microservices
- Traditional style -  Multiple endpoints in one service
  public class UserService {

   ComposableFuture<Response> createUser(Request request) {
         // Do stuff
    }

   ComposableFuture<Response> getUser(Request request) {
          // Do stuff
    }

   ComposableFuture<Response> updateUser(Request request) {
          // Do stuff
    }
  }

- In servervless you break up *endpoints* (functions) into isolated deployments


* Fault Tolerance
- Each "request" is protected; Running in its own linux namespaces (e.g. docker)
- Fatal failures do not bring down a service or a server (out of disk space)
- No restarting; No OutOfMemory; No GC pauses that affects other requests

In Serverless, there are no *thread* *pools*, you are bound by the number of available servers.

* Offerings
- AWS lambda (2015)
- Google Cloud Functions
- Azure Functions
- IBM openwhisk

* Examples 


* Example - AdServer
Let’s think about an online ad system - when a user clicks on an advertisement you want to very quickly redirect them to the target of the ad, but at the same time you need to collect the fact that the click has happened so that you can charge the advertiser. 

.image ad_server_click.svg

* Example - Image thumbnail
.background image_thumbnail.png
.html image_thumbnail.html


* What's really going on?
- Your code and dependencies are on local storage
- A generic docker container *mounts* that code and runs something like this:
  #!/bin/bash
  # set enviroment variables (especially Main-Class)
  # set up logger, metrics
  ... 
  docker run -p :$_SERVER_PORT \ 
    --env-file faas.env \
    --volume /local/path/to/code:/var/task --memory 512 \
    amazonlinux:2017.03 bootstrap-main-wrapper.sh
  ...
- Capture exit code
- Send logs & Bill!
: Recall tweet

* I code therefore I think
.code -numbers example1.go

- compile, zip, upload
- register an "event" to trigger this code

But what's going on with `lambda.Start()` ?

* Warm/Cold invocation
.code -numbers start.go
- RPC server
- Docker maps `_SERVER_PORT` on *host* server into the container

* Putting it all together 
: Nginix public HTTPS server 
: Controller - gatekeeper of the system orchestrator
: CouchDB  - credentials, metadata, namespaces, and the definitions of actions, triggers, and rules
: Consul - State management, service discovery
: Invoker - spins up docker & unit of execution for the chosen Action
.image whisk.png 600 1000

* What is AWS Lambda?
AWS Lambda is an *event-driven* compute service that runs code
in response to events and automatically manages the compute
resources required by that code

It is *STATELESS* and is *terminated* after completion.

* Which events?
.background events.png
- Manual
- API Gateway (HTTP requests)
- CloudFormation
- Code Commit
- Congito
- DynamoDB
- Kinesis
- S3
- SNS
- AutoScalling

* How do I use it?
- Pick a language (please no java)
- Import AWS SDK dependency
- Write a handler
.image southpark.png 400 800


* How do I deploy?
- Build/Compile code
- Create zip with all dependencies
- Use AWS console or AWS CLI to create lambda

  aws lambda create-function \ 
    --region us-west-2 \
    --function-name lambdafn \
    --zip-file mycode.zip \
    --handler main.helloword \
    --runtime go1.x  \

* How do I invoke?
AWS console or AWS cli

  aws lambda invoke \
    --invocation-type RequestResponse \
    --function-name lambdash-function-XINM2V7BQ5I8   \
    --log-type Tail \
    --payload '{"cmd": "echo Hello World"}'
    
  {
    "LogResult": "U1RBUlQgUmVxdWVzdElkOiAyMjg2Njg5My0yMTg5LTExZTgtOTIxZS1lZjAwNDVjNzRkMzEgVmVyc2lvbjogJExBVEVTVApFTkQgUmVxdWVzdElkOiAyMjg2Njg5My0yMTg5LTExZTgtOTIxZS1lZjAwNDVjNzRkMzEKUkVQT1JUIFJlcXVlc3RJZDogMjI4NjY4OTMtMjE4OS0xMWU4LTkyMWUtZWYwMDQ1Yzc0ZDMxCUR1cmF0aW9uOiAzMDQuMDIgbXMJQmlsbGVkIER1cmF0aW9uOiA0MDAgbXMgCU1lbW9yeSBTaXplOiAxMjggTUIJTWF4IE1lbW9yeSBVc2VkOiAzOSBNQgkK", 
    "ExecutedVersion": "$LATEST", 
    "StatusCode": 200
  }

You can also do a dry run invocations
*Note* output log is base64 encoded

  base64 --decode


* Internals
- Limits
- OS & specs
- Environment
- Logging


* Limits
.html limits.html

* OS
- Linux kernel *4.9*.75-25.55.amzn1.x86_64
- AMI (disk image) [[https://console.aws.amazon.com/ec2/v2/home#Images:visibility=public-images;search=amzn-ami-hvm-2017.03.1.20170812-x86_64-gp2][amzn-ami-hvm-2017.03.1.20170812-x86_64-gp2]]
- 2x *Xeon* E5-2680 2.80GHz
- Most likely runs in docker containers
.image linux.png 100 100
.image docker.png  100 100


* Environment
.html env1.html

* Logging
(more details and screen shots)
- CloudWatch



* Pricing
- Requests
- Duration

*1,000,000* Free (as in beer) requests per month
*$0.20* per 1,000,000 request after ($0.0000002 per request)

Execution duration is charge based upon your lambda memory definition
If your lambda definition is *1024* *MB* of mem but you only use *128* *MB*
you will be _billed_ for 1024 MB

*$0.00001667* For Every GB-Second
*400,000* GB-Seconds per month free


#.html pricing.html

* Pricing Example
If you allocated *512MB* of memory to your function, executed it 3 million times in one month, and it ran for 1 second each time, your charges would be calculated as follows:

- Monthly compute/duration charges
The monthly compute price is $0.00001667 per GB-s
Total compute (GB-s) = 3M * (1s) * 512MB/1024 = 1,500,000 GB-s
Total compute – Free tier compute = Monthly billable compute GB- s
1,500,000 GB-s – 400,000 GB-s = 1,100,000 GB-s
Monthly compute charges = 1,100,000 * $0.00001667 = *$18.34*

- Monthly request charges
Total requests – Free tier requests * 0.2/M = Monthly billable requests
Monthly request charges = 3M-1M * $0.2/M = *$0.40*

Compute charges + Request charges = Total charges
$18.34 + $0.40 = *$18.74* per month

* Examples

* Show me the code
.code -numbers example2.go

* Best Practices
- Code as infrastructure
- Function alias
- Alerts (billing)
- Testing procedures

* Gotchas
- Logging
- Too many functions
- Permissions
- Default no internet

* The future and ecosystems
(TODO)

* Resources
- lambdash
- Openstack
- serverless
- aws (https://aws.amazon.com/documentation/lambda/)
[[https://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html][Aws docs]]
https://eu-central-1.console.aws.amazon.com/lambda/home?region=eu-central-1#/home
https://github.com/open-lambda/open-lambda
https://www.ibm.com/cloud/functions
https://github.com/apache/incubator-openwhisk
